{"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7105074,"sourceType":"datasetVersion","datasetId":4096079}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hate Speech and Offensive Language Detection\n- This scripts uses the hate speech detection model to classify a given text as hate speech, offensive speech or neither. \n- The model is trained using the dataset provided by the authors, im using LogisticRegression and DecisionTreeClassifier to train the model.\n- The are some imbalance in the dataset, so i tried to used SMOTE, ADASYN and SMOTENC to balance the dataset. \n- The output of this model still not good enough","metadata":{}},{"cell_type":"markdown","source":"## Library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\nimport pickle\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download the Data","metadata":{}},{"cell_type":"code","source":"# import dataset from kaggle\n!kaggle datasets download -d thedevastator/hate-speech-and-offensive-language-detection","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip hate-speech-and-offensive-language-detection.zip -d data_input","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Data","metadata":{}},{"cell_type":"code","source":"# load dataset\ndata = pd.read_csv('data_input/train.csv')\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for missing values\ndata.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for duplicates\ndata.duplicated().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check random tweet\ndata['tweet'][100]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean the data \n\nstop_words = set(stopwords.words('english'))\n# add some more stop words\nstop_words.add('rt')\n\n## remove special characters\ndef remove_special_char(text):\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\n## remove urls\ndef remove_urls(text):\n    return re.sub(r'http\\S+', '', text)\n\n## remove usernames and hashtags\ndef remove_usernames_hashtags(text):\n    return re.sub(r'@\\w+|#\\w+', '', text)\n\n## remove extra spaces\ndef remove_extra_spaces(text):\n    return re.sub(r'\\s+', ' ', text)\n\n##  clean the text\ndef clean_text(text):\n    text = text.lower()\n    text = remove_special_char(text)\n    text = remove_urls(text)\n    text = remove_usernames_hashtags(text)\n    text = remove_extra_spaces(text)\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply the cleaning function to the dataset\n\ndata['cleaned_tweet'] = data['tweet'].apply(clean_text)\n\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compared cleaned tweet with original tweet\n\ndata[['tweet', 'cleaned_tweet']].head(10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove stop words\n\ndef remove_stop_words(text):\n    return ' '.join([word for word in text.split() if word not in stop_words])\n\ndata['cleaned_tweet_two'] = data['cleaned_tweet'].apply(remove_stop_words)\n\ndata.head()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare cleaned tweet with original tweet\n\ndata[['tweet', 'cleaned_tweet', 'cleaned_tweet_two']].tail(10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove short words\n\ndef remove_short_words(text):\n    return ' '.join([word for word in text.split() if len(word) > 2])\n\ndata['cleaned_tweet_three'] = data['cleaned_tweet_two'].apply(remove_short_words)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['tweet', 'cleaned_tweet', 'cleaned_tweet_two', 'cleaned_tweet_three']].tail(10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the most common words in the dataset\n\nfrom collections import Counter\n\nwords = data['cleaned_tweet_three'].apply(lambda x: [word for word in x.split()])\n\n# most common words in the dataset (general)\nwords = [word for sublist in words for word in sublist]\nword_count = Counter(words)\nword_count.most_common(20)\n\n# visualize the most common words\nimport matplotlib.pyplot as plt\n\nword_count_df = pd.DataFrame(word_count.most_common(20), columns=['word', 'count'])\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x='word', y='count', data=word_count_df)\nplt.xticks(rotation=45)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same with before, but using word cloud\n\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(' '.join(words))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now im curios. how about the most common words for only the neutral class?\n# im assumming that neutral class is number 2 in the class column from looking a bit of the dataset\n\nneutral_tweets = data[data['class'] == 2]['cleaned_tweet_three']\nneutral_words = neutral_tweets.apply(lambda x: [word for word in x.split()])\n\nneutral_words = [word for sublist in neutral_words for word in sublist]\nneutral_word_count = Counter(neutral_words)\nneutral_word_count.most_common(20)\n\nwordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(' '.join(neutral_words))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the most common words in neutral tweets\n\nneutral_word_count_df = pd.DataFrame(neutral_word_count.most_common(20), columns=['word', 'count'])\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x='word', y='count', data=neutral_word_count_df)\nplt.xticks(rotation=45)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"well its quite unexpected, why does trash belong to neutral label? lets take a deeper look","metadata":{}},{"cell_type":"code","source":"# show thow many tweet that include the word trash in it\n\ndata[data['cleaned_tweet_three'].str.contains('trash')]\ndata[data['cleaned_tweet_three'].str.contains('trash')]['class'].value_counts()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show five random  full tweets that include the word trash in it\n\npd.set_option('display.max_colwidth', None)\nfull_tweets_with_word_trash = data[data['cleaned_tweet_three'].str.contains('trash')]['cleaned_tweet_three'].sample(5)\nprint(full_tweets_with_word_trash)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"after taking a quick look. there goes the exploratory data analysis.. i believed its kinda bias to the neutral class having a trash word on it, but i think its okay for now.","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# classify the tweets between hate speech, offensive language and neither\n\ntweet = list(data['cleaned_tweet_three'])\nlabel = list(data['class'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data into training and testing set\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nX_train, X_test, y_train, y_test = train_test_split(tweet, label, test_size=0.2, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vectorize the cleaned_tweet_three column\ntfidf = TfidfVectorizer(max_features=5000)\n\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see the shape of the vectorized data\n\nX_train_tfidf.shape, X_test_tfidf.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\n\nmodel = LogisticRegression()\nmodel.fit(X_train_tfidf, y_train)\n\n# make predictions\ny_pred = model.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pipeline the model with count vectorizer and logistic regression\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\n\npipe = Pipeline([\n    ('vectorizer', CountVectorizer(max_features=5000)),\n    ('model', LogisticRegression())\n])\n\npipe.fit(X_train, y_train)\n\n# make predictions\ny_pred_pipe = pipe.predict(X_test)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_pipe))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the model with a hate speech tweet\n\ntweet = ['I hate you']\n\ntweet_tfidf = tfidf.transform(tweet)\nmodel.predict(tweet_tfidf)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the model with a offensive speech tweet\n\ntweet = ['You are so stupid']\n\ntweet_tfidf = tfidf.transform(tweet)\nmodel.predict(tweet_tfidf)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the model with a neutral tweet\n\ntweet = ['I am learning data science']\n\ntweet_tfidf = tfidf.transform(tweet)\nmodel.predict(tweet_tfidf)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"i'm assumming based on the result that 0 is hate speech, 1 is offensive language and 2 is neutral","metadata":{}},{"cell_type":"code","source":"# based on the 2 previous model its prettr sure has really imbalance class by looking at the score value on class 0. i will try to use SMOTE to balance the class\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\n\nX_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model with the balanced data\n\nmodel_smote = LogisticRegression()\nmodel_smote.fit(X_train_smote, y_train_smote)\n\n# make predictions\ny_pred_smote = model_smote.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_smote))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"soo... the model did improve on the recall. but the value is still not good enough. i will try to use hyperparameter tuning to improve the model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'penalty': ['l1', 'l2']\n}\n\ngrid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid_search.fit(X_train_smote, y_train_smote)\n\n# best parameters\ngrid_search.best_params_\nprint('Best parameters', grid_search.best_params_)\n\n# best score\n\nprint('Best score', grid_search.best_score_)\n\n# make predictions\ny_pred_grid = grid_search.predict(X_test_tfidf)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred_grid))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"nvm the model got worse on the class 0 score. i will try different model","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train_smote, y_train_smote)\n\n# make predictions\ny_pred_rf = model_rf.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_rf))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"still not good enough. i will try to use another model\n","metadata":{}},{"cell_type":"code","source":"# improve the model imbalance class ADASYN\n\nfrom imblearn.over_sampling import ADASYN\n\nadasyn = ADASYN(random_state=42)\n\nX_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_tfidf, y_train)\n\n# train the model with the balanced data\n\nmodel_adasyn = RandomForestClassifier()\nmodel_adasyn.fit(X_train_adasyn, y_train_adasyn)\n\n# make predictions\ny_pred_adasyn = model_adasyn.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_adasyn))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In summary, the main difference between ADASYN and SMOTE is in their approach to oversampling\n\n- SMOTE generates synthetic samples uniformly across the feature space, \n- while ADASYN places more emphasis on generating synthetic samples in areas where the classification is difficult, thereby adapting the oversampling based on the datasetâ€™s specific challenges. but the result doesnt change much on the class 0 scores overall","metadata":{}},{"cell_type":"markdown","source":"now the **SMOTENC** (Synthetic Minority Over-sampling Technique for Nominal and Continuous features) is an extension of the SMOTE algorithm specifically designed to handle datasets with both categorical and numerical features.","metadata":{}},{"cell_type":"code","source":"# generate more data so the model can learn more on the minority class\n\nfrom imblearn.over_sampling import SMOTENC\n\nsmotenc = SMOTENC(categorical_features=[0], random_state=42)\n\nX_train_smotenc, y_train_smotenc = smotenc.fit_resample(X_train_tfidf, y_train)\n\n# train the model with the balanced data\nmodel_smotenc = RandomForestClassifier()\nmodel_smotenc.fit(X_train_smotenc, y_train_smotenc)\n\n# make predictions\ny_pred_smotenc = model_smotenc.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_smotenc))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary\n\n### First Model (Logistic Regression):\n|           | Precision | Recall | F1-Score | Support |\n|-----------|-----------|--------|----------|---------|\n| Class 0   | 0.49      | 0.16   | 0.24     | 290     |\n| Class 1   | 0.91      | 0.96   | 0.94     | 3832    |\n| Class 2   | 0.83      | 0.81   | 0.82     | 835     |\n|-----------|-----------|--------|----------|---------|\n| Accuracy  |           |        | 0.89     | 4957    |\n| Macro Avg | 0.75      | 0.64   | 0.66     | 4957    |\n| Weighted Avg | 0.87   | 0.89   | 0.88     | 4957    |\n\n\n### Model After SMOTENC (Random Forest):\n|           | Precision | Recall | F1-Score | Support |\n|-----------|-----------|--------|----------|---------|\n| Class 0   | 0.38      | 0.39   | 0.39     | 290     |\n| Class 1   | 0.94      | 0.92   | 0.93     | 3832    |\n| Class 2   | 0.78      | 0.85   | 0.81     | 835     |\n|-----------|-----------|--------|----------|---------|\n| Accuracy  |           |        | 0.87     | 4957    |\n| Macro Avg | 0.70      | 0.72   | 0.71     | 4957    |\n| Weighted Avg | 0.88   | 0.88   | 0.88     | 4957    |","metadata":{}},{"cell_type":"markdown","source":"- Precision: The precision values for class 0 decreased from 0.49 to 0.38 after applying SMOTENC. However, precision values for classes 1 and 2 remained relatively high in both models.\n\n- Recall: The recall values for class 0 slightly increased from 0.16 to 0.39 after applying SMOTENC, indicating an improvement in capturing true positive instances for this class. Recall values for classes 1 and 2 remained stable in both models.\n\n- F1-score: The F1-scores for class 0 increased from 0.24 to 0.39 after applying SMOTENC, reflecting the improvement in precision and recall for this class. F1-scores for classes 1 and 2 remained consistent in both models.\n\n- Accuracy: The overall accuracy decreased slightly from 0.89 to 0.87 after applying SMOTENC.","metadata":{}},{"cell_type":"code","source":"## class weighting to adjust the imbalance class\nmodel_weighted = RandomForestClassifier(class_weight='balanced')\n\nmodel_weighted.fit(X_train_tfidf, y_train)\n\n# make predictions\ny_pred_weighted = model_weighted.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_weighted))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"currently several improvements that i made doesnt help the score for the class 0. from ChaDGPT recommends me to use Ensemble Methods, including bagging, boosting or stacking to combine multiple classifiers","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create a bagging classifier\nbagging_classifier = BaggingClassifier(DecisionTreeClassifier( max_depth=100,random_state=42,criterion='entropy'),\n                                         n_estimators=100,random_state=42,)\n# Train the bagging classifier\nbagging_classifier.fit(X_train_tfidf, y_train)\n\n# Make predictions\ny_pred_bagging = bagging_classifier.predict(X_test_tfidf)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred_bagging))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n# Create an AdaBoost classifier\nadaboost_classifier = AdaBoostClassifier(DecisionTreeClassifier( max_depth=100,random_state=42,criterion='entropy'),\n                                         n_estimators=100,random_state=42,)\n\n# Train the AdaBoost classifier\nadaboost_classifier.fit(X_train_tfidf, y_train)\n\n# Make predictions\ny_pred_adaboost = adaboost_classifier.predict(X_test_tfidf)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred_adaboost))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# viualize the best model performance\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(pd.DataFrame(classification_report(y_test, y_pred_smotenc, output_dict=True)).iloc[:-1, :].T, annot=True)\nplt.title('DecisionTreeClassifier with SMOTENC')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving model","metadata":{}},{"cell_type":"code","source":"# Saving both the model and tfidf on the model folder \n\npickle.dump(model_smotenc, open('model/model_smotenc.pkl', 'wb'))\npickle.dump(tfidf, open('model/tfidf.pkl', 'wb'))","metadata":{},"execution_count":null,"outputs":[]}]}