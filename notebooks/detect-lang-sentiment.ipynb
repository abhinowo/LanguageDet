{"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7105074,"sourceType":"datasetVersion","datasetId":4096079}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hate Speech and Offensive Language Detection\n- This scripts uses the hate speech detection model to classify a given text as hate speech, offensive speech or neither. \n- The model is trained using the dataset provided by the authors, im using LogisticRegression and DecisionTreeClassifier to train the model.\n- The are some imbalance in the dataset, so i tried to used SMOTE, ADASYN and SMOTENC to balance the dataset. \n- The output of this model still not good enough","metadata":{}},{"cell_type":"markdown","source":"## Library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\nimport pickle\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download the Data","metadata":{}},{"cell_type":"code","source":"# import dataset from kaggle\n!kaggle datasets download -d thedevastator/hate-speech-and-offensive-language-detection","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip hate-speech-and-offensive-language-detection.zip -d data_input","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Data","metadata":{}},{"cell_type":"code","source":"# load dataset\ndata = pd.read_csv('data_input/train.csv')\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for missing values\ndata.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for duplicates\ndata.duplicated().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check random tweet\ndata['tweet'][100]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean the data \n\nstop_words = set(stopwords.words('english'))\n# add some more stop words\nstop_words.add('rt')\n\n## remove special characters\ndef remove_special_char(text):\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\n## remove urls\ndef remove_urls(text):\n    return re.sub(r'http\\S+', '', text)\n\n## remove usernames and hashtags\ndef remove_usernames_hashtags(text):\n    return re.sub(r'@\\w+|#\\w+', '', text)\n\n## remove extra spaces\ndef remove_extra_spaces(text):\n    return re.sub(r'\\s+', ' ', text)\n\n##  clean the text\ndef clean_text(text):\n    text = text.lower()\n    text = remove_special_char(text)\n    text = remove_urls(text)\n    text = remove_usernames_hashtags(text)\n    text = remove_extra_spaces(text)\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply the cleaning function to the dataset\n\ndata['cleaned_tweet'] = data['tweet'].apply(clean_text)\n\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compared cleaned tweet with original tweet\n\ndata[['tweet', 'cleaned_tweet']].head(10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove stop words\n\ndef remove_stop_words(text):\n    return ' '.join([word for word in text.split() if word not in stop_words])\n\ndata['cleaned_tweet_two'] = data['cleaned_tweet'].apply(remove_stop_words)\n\ndata.head()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare cleaned tweet with original tweet\n\ndata[['tweet', 'cleaned_tweet', 'cleaned_tweet_two']].tail(10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove short words\n\ndef remove_short_words(text):\n    return ' '.join([word for word in text.split() if len(word) > 2])\n\ndata['cleaned_tweet_three'] = data['cleaned_tweet_two'].apply(remove_short_words)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['tweet', 'cleaned_tweet', 'cleaned_tweet_two', 'cleaned_tweet_three']].tail(10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the most common words in the dataset\n\nfrom collections import Counter\n\nwords = data['cleaned_tweet_three'].apply(lambda x: [word for word in x.split()])\n\n# most common words in the dataset (general)\nwords = [word for sublist in words for word in sublist]\nword_count = Counter(words)\nword_count.most_common(20)\n\n# visualize the most common words\nimport matplotlib.pyplot as plt\n\nword_count_df = pd.DataFrame(word_count.most_common(20), columns=['word', 'count'])\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x='word', y='count', data=word_count_df)\nplt.xticks(rotation=45)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same with before, but using word cloud\n\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(' '.join(words))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now im curios. how about the most common words for only the neutral class?\n# im assumming that neutral class is number 2 in the class column from looking a bit of the dataset\n\nneutral_tweets = data[data['class'] == 2]['cleaned_tweet_three']\nneutral_words = neutral_tweets.apply(lambda x: [word for word in x.split()])\n\nneutral_words = [word for sublist in neutral_words for word in sublist]\nneutral_word_count = Counter(neutral_words)\nneutral_word_count.most_common(20)\n\nwordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(' '.join(neutral_words))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the most common words in neutral tweets\n\nneutral_word_count_df = pd.DataFrame(neutral_word_count.most_common(20), columns=['word', 'count'])\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x='word', y='count', data=neutral_word_count_df)\nplt.xticks(rotation=45)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"well its quite unexpected, why does trash belong to neutral label? lets take a deeper look","metadata":{}},{"cell_type":"code","source":"# show thow many tweet that include the word trash in it\n\ndata[data['cleaned_tweet_three'].str.contains('trash')]\ndata[data['cleaned_tweet_three'].str.contains('trash')]['class'].value_counts()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show five random  full tweets that include the word trash in it\n\npd.set_option('display.max_colwidth', None)\nfull_tweets_with_word_trash = data[data['cleaned_tweet_three'].str.contains('trash')]['cleaned_tweet_three'].sample(5)\nprint(full_tweets_with_word_trash)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"after taking a quick look. there goes the exploratory data analysis.. i believed its kinda bias to the neutral class having a trash word on it, but i think its okay for now.","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# classify the tweets between hate speech, offensive language and neither\n\ntweet = list(data['cleaned_tweet_three'])\nlabel = list(data['class'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data into training and testing set\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nX_train, X_test, y_train, y_test = train_test_split(tweet, label, test_size=0.2, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vectorize the cleaned_tweet_three column\ntfidf = TfidfVectorizer(max_features=5000)\n\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see the shape of the vectorized data\n\nX_train_tfidf.shape, X_test_tfidf.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\n\nmodel = LogisticRegression()\nmodel.fit(X_train_tfidf, y_train)\n\n# make predictions\ny_pred = model.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pipeline the model with count vectorizer and logistic regression\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\n\npipe = Pipeline([\n    ('vectorizer', CountVectorizer(max_features=5000)),\n    ('model', LogisticRegression())\n])\n\npipe.fit(X_train, y_train)\n\n# make predictions\ny_pred_pipe = pipe.predict(X_test)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_pipe))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the model with a hate speech tweet\n\ntweet = ['I hate you']\n\ntweet_tfidf = tfidf.transform(tweet)\nmodel.predict(tweet_tfidf)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the model with a offensive speech tweet\n\ntweet = ['You are so stupid']\n\ntweet_tfidf = tfidf.transform(tweet)\nmodel.predict(tweet_tfidf)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the model with a neutral tweet\n\ntweet = ['I am learning data science']\n\ntweet_tfidf = tfidf.transform(tweet)\nmodel.predict(tweet_tfidf)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"i'm assumming based on the result that 0 is hate speech, 1 is offensive language and 2 is neutral","metadata":{}},{"cell_type":"code","source":"# based on the 2 previous model its prettr sure has really imbalance class by looking at the score value on class 0. i will try to use SMOTE to balance the class\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\n\nX_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model with the balanced data\n\nmodel_smote = LogisticRegression()\nmodel_smote.fit(X_train_smote, y_train_smote)\n\n# make predictions\ny_pred_smote = model_smote.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_smote))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"soo... the model did improve on the recall. but the value is still not good enough. i will try to use hyperparameter tuning to improve the model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'penalty': ['l1', 'l2']\n}\n\ngrid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid_search.fit(X_train_smote, y_train_smote)\n\n# best parameters\ngrid_search.best_params_\nprint('Best parameters', grid_search.best_params_)\n\n# best score\n\nprint('Best score', grid_search.best_score_)\n\n# make predictions\ny_pred_grid = grid_search.predict(X_test_tfidf)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred_grid))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"nvm the model got worse on the class 0 score. i will try different model","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train_smote, y_train_smote)\n\n# make predictions\ny_pred_rf = model_rf.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_rf))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"still not good enough. i will try to use another model\n","metadata":{}},{"cell_type":"code","source":"# improve the model imbalance class ADASYN\n\nfrom imblearn.over_sampling import ADASYN\n\nadasyn = ADASYN(random_state=42)\n\nX_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_tfidf, y_train)\n\n# train the model with the balanced data\n\nmodel_adasyn = RandomForestClassifier()\nmodel_adasyn.fit(X_train_adasyn, y_train_adasyn)\n\n# make predictions\ny_pred_adasyn = model_adasyn.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_adasyn))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In summary, the main difference between ADASYN and SMOTE is in their approach to oversampling\n\n- SMOTE generates synthetic samples uniformly across the feature space, \n- while ADASYN places more emphasis on generating synthetic samples in areas where the classification is difficult, thereby adapting the oversampling based on the dataset’s specific challenges. but the result doesnt change much on the class 0 scores overall","metadata":{}},{"cell_type":"markdown","source":"now the **SMOTENC** (Synthetic Minority Over-sampling Technique for Nominal and Continuous features) is an extension of the SMOTE algorithm specifically designed to handle datasets with both categorical and numerical features.","metadata":{}},{"cell_type":"code","source":"# generate more data so the model can learn more on the minority class\n\nfrom imblearn.over_sampling import SMOTENC\n\nsmotenc = SMOTENC(categorical_features=[0], random_state=42)\n\nX_train_smotenc, y_train_smotenc = smotenc.fit_resample(X_train_tfidf, y_train)\n\n# train the model with the balanced data\nmodel_smotenc = RandomForestClassifier()\nmodel_smotenc.fit(X_train_smotenc, y_train_smotenc)\n\n# make predictions\ny_pred_smotenc = model_smotenc.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_smotenc))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary\n\n### First Model (Logistic Regression):\n|           | Precision | Recall | F1-Score | Support |\n|-----------|-----------|--------|----------|---------|\n| Class 0   | 0.49      | 0.16   | 0.24     | 290     |\n| Class 1   | 0.91      | 0.96   | 0.94     | 3832    |\n| Class 2   | 0.83      | 0.81   | 0.82     | 835     |\n|-----------|-----------|--------|----------|---------|\n| Accuracy  |           |        | 0.89     | 4957    |\n| Macro Avg | 0.75      | 0.64   | 0.66     | 4957    |\n| Weighted Avg | 0.87   | 0.89   | 0.88     | 4957    |\n\n\n### Model After SMOTENC (Random Forest):\n|           | Precision | Recall | F1-Score | Support |\n|-----------|-----------|--------|----------|---------|\n| Class 0   | 0.38      | 0.39   | 0.39     | 290     |\n| Class 1   | 0.94      | 0.92   | 0.93     | 3832    |\n| Class 2   | 0.78      | 0.85   | 0.81     | 835     |\n|-----------|-----------|--------|----------|---------|\n| Accuracy  |           |        | 0.87     | 4957    |\n| Macro Avg | 0.70      | 0.72   | 0.71     | 4957    |\n| Weighted Avg | 0.88   | 0.88   | 0.88     | 4957    |","metadata":{}},{"cell_type":"markdown","source":"- Precision: The precision values for class 0 decreased from 0.49 to 0.38 after applying SMOTENC. However, precision values for classes 1 and 2 remained relatively high in both models.\n\n- Recall: The recall values for class 0 slightly increased from 0.16 to 0.39 after applying SMOTENC, indicating an improvement in capturing true positive instances for this class. Recall values for classes 1 and 2 remained stable in both models.\n\n- F1-score: The F1-scores for class 0 increased from 0.24 to 0.39 after applying SMOTENC, reflecting the improvement in precision and recall for this class. F1-scores for classes 1 and 2 remained consistent in both models.\n\n- Accuracy: The overall accuracy decreased slightly from 0.89 to 0.87 after applying SMOTENC.","metadata":{}},{"cell_type":"code","source":"## class weighting to adjust the imbalance class\nmodel_weighted = RandomForestClassifier(class_weight='balanced')\n\nmodel_weighted.fit(X_train_tfidf, y_train)\n\n# make predictions\ny_pred_weighted = model_weighted.predict(X_test_tfidf)\n\n# evaluate the model\nprint(classification_report(y_test, y_pred_weighted))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"currently several improvements that i made doesnt help the score for the class 0. from ChaDGPT recommends me to use Ensemble Methods, including bagging, boosting or stacking to combine multiple classifiers","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create a bagging classifier\nbagging_classifier = BaggingClassifier(DecisionTreeClassifier( max_depth=100,random_state=42,criterion='entropy'),\n                                         n_estimators=100,random_state=42,)\n# Train the bagging classifier\nbagging_classifier.fit(X_train_tfidf, y_train)\n\n# Make predictions\ny_pred_bagging = bagging_classifier.predict(X_test_tfidf)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred_bagging))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n# Create an AdaBoost classifier\nadaboost_classifier = AdaBoostClassifier(DecisionTreeClassifier( max_depth=100,random_state=42,criterion='entropy'),\n                                         n_estimators=100,random_state=42,)\n\n# Train the AdaBoost classifier\nadaboost_classifier.fit(X_train_tfidf, y_train)\n\n# Make predictions\ny_pred_adaboost = adaboost_classifier.predict(X_test_tfidf)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred_adaboost))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# viualize the best model performance\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(pd.DataFrame(classification_report(y_test, y_pred_smotenc, output_dict=True)).iloc[:-1, :].T, annot=True)\nplt.title('DecisionTreeClassifier with SMOTENC')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving model","metadata":{}},{"cell_type":"code","source":"# Saving both the model and tfidf on the model folder \n\npickle.dump(model_smotenc, open('model/model_smotenc.pkl', 'wb'))\npickle.dump(tfidf, open('model/tfidf.pkl', 'wb'))","metadata":{},"execution_count":null,"outputs":[]}]}